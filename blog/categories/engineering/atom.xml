<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Engineering | Miker]]></title>
  <link href="http://rowehl.com/blog/categories/engineering/atom.xml" rel="self"/>
  <link href="http://rowehl.com/"/>
  <updated>2013-05-29T07:13:02-07:00</updated>
  <id>http://rowehl.com/</id>
  <author>
    <name><![CDATA[Mike Rowehl]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Consistent Test Methodology for Inconsistent Projects]]></title>
    <link href="http://rowehl.com/blog/2013/05/28/consistent-test-methodology-for-inconsistent-projects/"/>
    <updated>2013-05-28T23:43:00-07:00</updated>
    <id>http://rowehl.com/blog/2013/05/28/consistent-test-methodology-for-inconsistent-projects</id>
    <content type="html"><![CDATA[<p>How do you start introducing some testing if you have a huge group of existing
projects, for the most part all implemented using different languages and
technologies? That's the problem I've been poking at recently. The first issue
is that none of the technology choices were made with testability in mind. And
I don't want to have to go through and run a bunch of code refactoring and
reorganization just to start testing. I would much rather start testing, and
then start introducing changes to make things easier to test and to increase
the coverage. The second issue is that we're pretty sure there are some major
architectural changes and redesigns coming in the short term. So to dig in and
harness up the guts of a bunch of systems we know will be changing really
shortly anyway also doesn't seem like a fantastic idea. Plus some of the
changes are more about the operating environment than the code itself, and
a bunch of unit tests won't necessarily help us figure out if the web services
break when we move from MySQL 5.5 to 5.6.</p>

<p>The most obvious and brute-force way would be to recreate the whole end-to-end
environment and create some system tests for it. I think that would be too
brute-force though. I would like our first step to provide some tools that
would be useful to developers and the infrastructure folks. A whole end-to-end
environment might be useful for infrastructure, but would be a burden for
development. Even if this were running up on AWS, imagine having to spin up
a dozen instances and wait for them to initialize just so you could test your
one change. And the system as a whole definitely wasn't built with testability
at the granularity of the entire infrastructure in mind. So good luck being
able to hit even a small set of the conditions the running systems are
subjected to.</p>

<p>No, what we needed to start were a few automated acceptance tests at the
granularity of the individual architecture components. Fortunately, cause much
of the process before was run off manual QA there are collections of test
cases floating around for lots of the components. And there's frequently even
a description on a wiki page of how to configure a QA environment for a given
component (for example how to setup a database, configure Tomcat, and install
and run a web service). So now it's just a matter of being able to script all
that up.</p>

<p>Since I wanted these tests to be useful both for development time checks and
infrastructure validation that meant they needed to run on an environment as
close to production as possible. But setting up a bunch of testing
environments introduces a whole other set of problems potentially as large as
the set we're trying to solve. Spinning up the instances in the cloud might
work, but also introduces a different level of complexity. I wanted
something that a developer could have available right in their project
checkout they could use to validate their work as they go along and ensure
there won't be any surprises when trying to run in production. Fortunately,
that's exactly what <a href="http://www.vagrantup.com/">Vagrant</a> is meant to provide.</p>

<p>With Vagrant and <a href="https://www.virtualbox.org/">VirtualBox</a> on a development
machine it's easy to spawn a VM running the same version of CentOS we're
running in production. The instructions for setting up a QA environment
become
<a href="http://docs.vagrantup.com/v2/provisioning/index.html">provisioning scripts</a>
for the Vagrant config, which automatically get run when the VM launches.
The automation for the test cases themselves or loading data need to be
handled on a case-by-case basis, but with the ability to sync files between
host system and VM and automatically configure port forwarding into the VM
it makes it much simpler to bundle everything into a single command.</p>

<p>Cause we're testing at the same granularity at which these projects are
deployed, we don't have to refactor the projects before we can
start automating some testing. The projects already have well defined input
and output points, which is how they communicate with the rest of the
infrastructure, so we just latch onto those. Now, admittedly, the service
contracts for lots of the projects are fuzzy at best. Cleaning those up is one
of our medium term goals. But at least now we can start with testing in place
and then start changing stuff.</p>

<p>And if the infrastructure folks want to try running on a different distro,
or using a different version of a package, they also now have the tools to
check changes without having to run things through QA.
Make the necessary changes on the VM and then run the script to make sure
nothing has broken.</p>

<p>For this to start looking like a proper continuous delivery test setup we
should be driving the setup of the VM using the same configuration as we use
in production, which is why we're also working on swapping to a more robust
infrastructure automation system than we have currently. For example, Vagrant
allows Chef or Puppet for provisioning instead of using a shell script. And
instead of using the synced filesystem to move data back and forth, the system
under test should be deployed using the same tools and process as the deploy
to production. But at least there's movement in the right direction now.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rebooting an Engineering Organization]]></title>
    <link href="http://rowehl.com/blog/2013/05/25/rebooting-an-engineering-organization/"/>
    <updated>2013-05-25T00:06:00-07:00</updated>
    <id>http://rowehl.com/blog/2013/05/25/rebooting-an-engineering-organization</id>
    <content type="html"><![CDATA[<p>When Metaresolver got purchased by Millennial Media I figured I would have
some bits of strategic input around how to approach the evolving mobile media
models, and then I would help hack through some of the prototypes and early
systems in those new lines of business. That version was interesting cause of
the market position that Millennial has, and because it's been a really long
time since I've worked with a group larger than 10 people. But I had assumed
that the challenging part was going to be keeping myself from getting
overly bored by the rate of progress so I could hang out long enough to
make a difference.</p>

<p>And then when our deal actually formally closed the founding CTO from
Millennial left at the same time. In the wake of
that the decision was made: it was time for a major restructuring on the
technology side of Millennial. The company has a tremendous market position,
but we've been having trouble capitalizing on it. We needed to start
updating the way Millennial practices engineering so that it could keep at
the front of a rapidly evolving market. Suddenly being bored wasn't on my
list of concerns any more.</p>

<p>The part of the reboot that I've been most actively involved in is down in the
technical details of revamping how engineers get their daily work done. There
are some really simple parts that everyone has already been talking about
forever, and just needed some focused attention. Not everything is running
through a continuous integration process, and lots of it doesn't have
automated testing it can be run through. The organization as a whole has
fallen so far into the antipatterns of manual QA and the
<a href="http://www.rendell.org/pebble/software/2010/02/09/1265756844985.html">high ceremony release vehicle</a>
that there's a ton of resistance around fixing the deployment process. But
fortunately it's 2013 now, even in Baltimore. So hopefully it's just a matter
of time before everyone understands the value of getting working code up into
production quickly, and that there are better ways to make sure we enforce the
"working code" bit than throwing bodies and forms at the issue.</p>

<p>The overall goal is to get us to something that looks much more like a
continuous delivery process. There are some patterns that are just so second
nature to me and everyone I've worked with over the last few years that I
frequently forget they're not universal yet. Everything I've worked on over
the last few years has evolved in relatively the same direction: deployments
are single script (or single git push) affairs, the scripts write some
info into a shared channel so that people know what went out, the operational
monitors are visible to everyone and well defined, and rolling back if those
monitors go sideways is always part of the plan. It's awesome.</p>

<p>So when I ask someone about deployment tools and they start to describe
something that has a cabal of approvers meeting to bless things and change
notification forms I can only stand there stunned. Well, first I throw up in
my mouth a little bit, and then I stand there stunned.</p>

<p>Guess this is what I get for leaving Silicon Valley without proper backup.
It's a backwards and fucked up place out here my friends, and I can't
say I recommend it at all. But I've started to pull together the
necessary backup and my plan is to fix at least a small part of it before
my next startup. You're welcome.</p>
]]></content>
  </entry>
  
</feed>
