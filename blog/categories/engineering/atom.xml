<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Engineering | Miker]]></title>
  <link href="http://rowehl.com/blog/categories/engineering/atom.xml" rel="self"/>
  <link href="http://rowehl.com/"/>
  <updated>2013-07-14T18:40:45-07:00</updated>
  <id>http://rowehl.com/</id>
  <author>
    <name><![CDATA[Mike Rowehl]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Scaling Technology]]></title>
    <link href="http://rowehl.com/blog/2013/07/14/scaling-technology/"/>
    <updated>2013-07-14T17:41:00-07:00</updated>
    <id>http://rowehl.com/blog/2013/07/14/scaling-technology</id>
    <content type="html"><![CDATA[<p>A few weeks ago Arte and Mario asked me to swing by to chat with folks participating in
<a href="http://momentum.mobilemonday.us/">the Momentum accelerator</a> to talk about
scaling technology. While we were talking I pointed folks to a few posts and videos of
talks I consider to be some of the root nodes of a lot of other conversations. I'm not
sure I've ever pulled this together before.</p>

<ul>
<li><a href="http://blip.tv/oreilly-velocity-conference/velocity-09-john-allspaw-10-deploys-per-day-dev-and-ops-cooperation-at-flickr-2297883">Allspaw/Hammond - 10+ Deploys Per Day</a></li>
<li><a href="http://www.slideshare.net/deimos/randy-shoup-ebays-architectural-principles">Randy Shoup - Ebay's Architectural Principles</a></li>
<li><a href="http://akfpartners.com/techblog/2008/05/08/splitting-applications-or-services-for-scale/">AKF Partners - Splitting Applications or Services for Scale</a></li>
<li><a href="http://highscalability.com/">High Scalability</a></li>
<li><a href="http://techblog.netflix.com/">Netflix Tech Blog</a></li>
</ul>


<p>That Allspaw/Hammond presentation is the most primary in my opinion. It's the talk
that lots of us consider to be the inflection point of the devops movement, when the
practice started to really gain visibility. What had previously been a random
collection of techniques we didn't have a great way to refer to became "devops".
There's a ton of useful information in that presentation. Not just the overall
concepts of dev and ops working together with a different kind of coupling, but some
specific tools to make that happen like shared metrics, feature flags, and shipping
dark.</p>

<p>The presentation from Randy Shoup has also withstood the test of time fantastically
well. The four architectural principles are great top level items: partition
everything, async everywhere, automate everything, remember everything fails. And the
presentation from AKF Partners dives into a bunch more detail about how to partition
services so that you can just throw hardware at problems. That model also provides a
lot of the basis for most of the conversations I end up having about geographic
distribution of services (which actually sounds exactly like a discussion about how
to design the I/O layer of an operating system, you just swap terminology).</p>

<p>The High Scalability and Netflix blogs provide great concrete examples of what some
of the problems and solutions look like. Principles are great, but like always
there's more than one way to do it. So it's great to have some concrete examples to
poke through. Plus Netflix being one of the pioneers of the static AMI methodology
of infrastructure automation it makes their approaches particularly strongly
opinionated.</p>

<p>The platforms themselves have changed the level of abstraction you need to work at
when putting this stuff into practice. Thanks to MongoDB and Riak we don't have to
wrapper tons of instances of MySQL or Postgres to make a datastore that scales
horizontally. And thanks to services like Elastic Beanstalk, Heroku, and EngineYard
we don't have to pay as much attention to how to distribute load across multiple
application servers. But even though you can count on your platform to remove a
bunch of the details in some cases, it's a good idea to understand the principles.
First of all cause it's possible to misuse the platform and end up not
getting the full advantage you could. But also because platforms can only help out
so far. Eventually you need to crack open the abstractions and either continue
with an application optimized line of evolution, or figure out how to wrapper
additional layers of capability around what the base platform provide. In either
case at least being aware of the principles your underlying platforms are working
on will pay back hugely.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Consistent Test Methodology for Inconsistent Projects]]></title>
    <link href="http://rowehl.com/blog/2013/05/28/consistent-test-methodology-for-inconsistent-projects/"/>
    <updated>2013-05-28T23:43:00-07:00</updated>
    <id>http://rowehl.com/blog/2013/05/28/consistent-test-methodology-for-inconsistent-projects</id>
    <content type="html"><![CDATA[<p>How do you start introducing some testing if you have a huge group of existing
projects, for the most part all implemented using different languages and
technologies? That's the problem I've been poking at recently. The first issue
is that none of the technology choices were made with testability in mind. And
I don't want to have to go through and run a bunch of code refactoring and
reorganization just to start testing. I would much rather start testing, and
then start introducing changes to make things easier to test and to increase
the coverage. The second issue is that we're pretty sure there are some major
architectural changes and redesigns coming in the short term. So to dig in and
harness up the guts of a bunch of systems we know will be changing really
shortly anyway also doesn't seem like a fantastic idea. Plus some of the
changes are more about the operating environment than the code itself, and
a bunch of unit tests won't necessarily help us figure out if the web services
break when we move from MySQL 5.5 to 5.6.</p>

<p>The most obvious and brute-force way would be to recreate the whole end-to-end
environment and create some system tests for it. I think that would be too
brute-force though. I would like our first step to provide some tools that
would be useful to developers and the infrastructure folks. A whole end-to-end
environment might be useful for infrastructure, but would be a burden for
development. Even if this were running up on AWS, imagine having to spin up
a dozen instances and wait for them to initialize just so you could test your
one change. And the system as a whole definitely wasn't built with testability
at the granularity of the entire infrastructure in mind. So good luck being
able to hit even a small set of the conditions the running systems are
subjected to.</p>

<p>No, what we needed to start were a few automated acceptance tests at the
granularity of the individual architecture components. Fortunately, cause much
of the process before was run off manual QA there are collections of test
cases floating around for lots of the components. And there's frequently even
a description on a wiki page of how to configure a QA environment for a given
component (for example how to setup a database, configure Tomcat, and install
and run a web service). So now it's just a matter of being able to script all
that up.</p>

<p>Since I wanted these tests to be useful both for development time checks and
infrastructure validation that meant they needed to run on an environment as
close to production as possible. But setting up a bunch of testing
environments introduces a whole other set of problems potentially as large as
the set we're trying to solve. Spinning up the instances in the cloud might
work, but also introduces a different level of complexity. I wanted
something that a developer could have available right in their project
checkout they could use to validate their work as they go along and ensure
there won't be any surprises when trying to run in production. Fortunately,
that's exactly what <a href="http://www.vagrantup.com/">Vagrant</a> is meant to provide.</p>

<p>With Vagrant and <a href="https://www.virtualbox.org/">VirtualBox</a> on a development
machine it's easy to spawn a VM running the same version of CentOS we're
running in production. The instructions for setting up a QA environment
become
<a href="http://docs.vagrantup.com/v2/provisioning/index.html">provisioning scripts</a>
for the Vagrant config, which automatically get run when the VM launches.
The automation for the test cases themselves or loading data need to be
handled on a case-by-case basis, but with the ability to sync files between
host system and VM and automatically configure port forwarding into the VM
it makes it much simpler to bundle everything into a single command.</p>

<p>Cause we're testing at the same granularity at which these projects are
deployed, we don't have to refactor the projects before we can
start automating some testing. The projects already have well defined input
and output points, which is how they communicate with the rest of the
infrastructure, so we just latch onto those. Now, admittedly, the service
contracts for lots of the projects are fuzzy at best. Cleaning those up is one
of our medium term goals. But at least now we can start with testing in place
and then start changing stuff.</p>

<p>And if the infrastructure folks want to try running on a different distro,
or using a different version of a package, they also now have the tools to
check changes without having to run things through QA.
Make the necessary changes on the VM and then run the script to make sure
nothing has broken.</p>

<p>For this to start looking like a proper continuous delivery test setup we
should be driving the setup of the VM using the same configuration as we use
in production, which is why we're also working on swapping to a more robust
infrastructure automation system than we have currently. For example, Vagrant
allows Chef or Puppet for provisioning instead of using a shell script. And
instead of using the synced filesystem to move data back and forth, the system
under test should be deployed using the same tools and process as the deploy
to production. But at least there's movement in the right direction now.</p>
]]></content>
  </entry>
  
</feed>
